
# [논문 요약] NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model

## 1. 논문 정보
- **제목**: NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model
- **저자**: NVIDIA (상세 기여자는 논문 마지막 "Contributors" 섹션에 팀별로 명시됨)
- **발표일**: 2025년 8월 18일

## 2. 연구 목적 및 배경
### 연구 목적
이 연구의 핵심 목표는 **추론(reasoning) 워크로드에 대한 처리량(throughput)을 극대화**하면서, 동시에 동급 크기의 모델 대비 **최고 수준의 정확도(state-of-the-art accuracy)를 달성**하는 하이브리드 언어 모델을 개발하는 것입니다. 구체적으로, 단일 NVIDIA A10G GPU(22GiB 메모리)에서 최대 128k 토큰의 컨텍스트를 처리할 수 있는 효율적인 모델을 만드는 것을 목표로 합니다.

### 배경
- **Nemotron-H 아키텍처 계승**: 이 모델은 기존의 Nemotron-H 아키텍처를 기반으로 합니다. 이 아키텍처는 일반적인 Transformer 구조의 `self-attention` 레이어 대부분을 `Mamba-2` 레이어로 대체하여, 추론에 필요한 긴 사고 과정(thinking traces) 생성 시 추론 속도를 향상시키는 특징을 가집니다.
- **효율성의 필요성**: 대규모 언어 모델(LLM)이 점점 더 복잡한 추론 작업을 수행하게 되면서, 긴 컨텍스트를 처리하고 긴 답변을 생성하는 데 드는 계산 비용과 시간이 중요한 문제로 대두되었습니다. Nemotron Nano 2는 이러한 **생성 중심(generation-heavy) 시나리오**에서 효율성을 높이기 위해 설계되었습니다.

## 3. 방법론
Nemotron Nano 2는 **사전 학습(Pre-training) → 정렬(Alignment) → 압축(Compression)**의 3단계 과정을 거쳐 개발되었습니다.

### 1단계: 사전 학습 (Pre-training)
1.  **초기 베이스 모델**: 먼저 120억(12B) 파라미터 크기의 `Nemotron-Nano-12B-v2-Base` 모델을 **20조(trillion) 토큰**의 대규모 데이터셋으로 사전 학습합니다.
2.  **FP8 학습**: 학습 효율을 높이기 위해 FP8 정밀도를 사용한 학습 레시피를 적용했습니다.
3.  **데이터 혼합 및 커리큘럼**: 학습 데이터는 웹 크롤링 데이터, 수학, 코드, 학술 자료 등 큐레이션된 데이터와 합성 생성 데이터를 혼합하여 사용했습니다. 특히, 품질에 따라 데이터의 가중치를 다르게 부여하고, 학습 단계를 3개의 Phase로 나누어 데이터 혼합 비율을 동적으로 변경하는 **학습 커리큘럼**을 적용했습니다.
4.  **장문 컨텍스트 확장**: 사전 학습 마지막 단계에서 512k 토큰의 시퀀스 길이로 지속적인 사전 학습(CPT, Continuous Pre-training)을 진행하여 모델이 128k의 긴 컨텍스트를 처리할 수 있도록 능력을 확장했습니다.

### 2단계: 정렬 (Alignment)
사전 학습된 베이스 모델이 사용자의 지시를 더 잘 따르고 유용하게 상호작용하도록 만드는 과정입니다.
1.  **다단계 SFT (Supervised Fine-Tuning)**: 약 900억 토큰의 프롬프트-응답 쌍으로 SFT를 수행했습니다. 도메인별(수학, 코딩, 도구 사용 등)로 SFT 단계를 나누고, 특히 긴 컨텍스트 및 예산 제어(budget control)를 위한 SFT를 추가로 진행했습니다.
2.  **선호도 최적화 (Preference Optimization)**:
    * **GRPO (Group Relative Policy Optimization)**: 지시 사항 준수 및 대화 능력을 향상시켰습니다.
    * **DPO (Direct Preference Optimization)**: 도구 사용(tool use) 능력을 강화했습니다.
    * **RLHF (Reinforcement Learning from Human Feedback)**: 전반적인 유용성과 채팅 능력을 개선했습니다.
3.  **모델 병합 (Model Merging)**: 추론 능력에 강점을 보이는 체크포인트와 채팅 능력에 강점을 보이는 체크포인트를 선형적으로 보간(interpolate)하여 두 장점을 모두 갖춘 최종 모델을 만들었습니다.

### 3단계: 압축 (Pruning and Distillation)
정렬된 12B 모델을 A10G GPU에 배포 가능한 9B 모델로 압축하는 과정입니다.
1.  **Minitron 프레임워크 활용**: 경량 모델 가지치기(pruning) 프레임워크인 Minitron을 확장하여 사용했습니다.
2.  **중요도 측정 (Importance Estimation)**: 모델의 각 구성 요소(레이어, FFN 뉴런 등)가 예측에 미치는 영향을 MSE(Mean Squared Error) 등으로 측정하여 제거할 대상을 결정했습니다.
3.  **NAS (Neural Architecture Search)**: 메모리 제약(19.66GiB)을 만족하는 수백 개의 후보 아키텍처 중, 깊이(depth)와 너비(width)를 조절하며 최적의 아키텍처를 탐색했습니다. 최종적으로 56개의 레이어를 가진 모델이 선택되었습니다.
4.  **지식 증류 (Knowledge Distillation)**: Pruning으로 인해 손실된 성능을 복구하기 위해, 원본 12B 모델(Teacher)의 출력을 9B 모델(Student)이 모방하도록 학습시키는 지식 증류를 수행했습니다.

## 4. 실험 설정 및 주요 결과
### 실험 설정
- **비교 모델**: 주로 Qwen3-8B와 성능 및 처리량을 비교합니다.
- **평가 벤치마크**:
    - **수학/추론**: AIME24/25, MATH, GSM8K, GPQA-D
    - **코딩**: LiveCodeBench, HumanEval+, MBPP+
    - **도구 사용**: BFCLv3
    - **장문 컨텍스트**: RULER 128k
    - **지시 사항 준수**: IFBench
- **처리량 측정**: A10G GPU에서 bfloat16 정밀도로 측정했으며, 8k 토큰 입력 / 16k 토큰 출력과 같은 생성 중심 시나리오에서 vLLM을 사용하여 측정했습니다.

### 주요 결과
- **처리량 및 정확도**: **Nemotron-Nano-9B-v2**는 Qwen3-8B에 비해 추론 벤치마크에서 **동등하거나 더 나은 정확도**를 보이면서, 생성 중심 시나리오(예: 8k 입력/16k 출력)에서 **최대 6.3배 높은 추론 처리량**을 달성했습니다. (Figure 1 참고)
- **베이스 모델 성능**: 압축된 9B 베이스 모델(`N-Nano-V2-9B-Base`)은 Qwen3-8B-Base와 Gemma3-12B-Base와 비교했을 때, 특히 MATH, AIME, 코딩 벤치마크에서 뛰어난 성능을 보였습니다. (Table 5, 6 참고)
- **정렬 모델 성능**: 최종 정렬 모델은 AIME, MATH-500, GPQA, RULER 등 다양한 고난도 추론 벤치마크에서 Qwen3-8B 및 14B 모델과 경쟁력 있는 성능을 입증했습니다. (Table 8 참고)

## 5. 분석
- **사고 예산 제어 (Budget Control)**: 모델이 답변을 생성하기 전에 "생각하는" 데 사용할 토큰의 양을 제어할 수 있는 기능입니다. 학습 데이터에 의도적으로 **중단된 추론 과정(truncated reasoning traces)**을 포함시킨 결과, 예산이 적게 주어졌을 때도 모델이 출력을 잘 구성하고 안정적으로 작동하는 것을 확인했습니다. (Figure 5 참고)
- **데이터 Ablation Study**:
    - **다국어 데이터**: 여러 다국어 데이터 소스 중, 영어 QA 데이터를 번역한 합성 데이터(`DiverseQA-crawl`)가 Global-MMLU 벤치마크에서 가장 좋은 성능을 보여, 데이터 혼합 시 높은 가중치를 부여했습니다.
    - **기초 추론 SFT 데이터**: 분석적/논리적 추론에 초점을 맞춘 합성 SFT 데이터를 추가했을 때, 특히 고난도 벤치마크인 MMLU-Pro 점수가 44.24에서 56.36으로 크게 향상되어 데이터의 효과를 입증했습니다.
- **압축 전략 분석**: 모델 깊이를 56개 레이어 미만으로 줄이면 성능 저하가 컸으며, 현재의 압축률에서는 Mamba 헤드를 pruning하는 것보다 FFN과 임베딩 차원을 pruning하는 것이 정확도 보존에 더 효과적이었습니다.

## 6. 결론 및 시사점
### 결론
- Nemotron-Nano-9B-v2는 **Mamba-Transformer 하이브리드 아키텍처**와 **체계적인 압축 전략(Pruning & Distillation)**을 결합하여, **높은 추론 성능과 뛰어난 처리 효율성**을 동시에 달성한 모델입니다.
- 이 모델은 단일 A10G GPU라는 제한된 하드웨어 환경에서도 128k의 긴 컨텍스트 추론을 가능하게 합니다.
- NVIDIA는 이 연구를 통해 최종 모델 2종(`9B-v2`, `9B-v2-Base`)과 부모 모델(`12B-v2-Base`), 그리고 사전 학습 및 후처리 데이터셋 대부분을 Hugging Face에 공개하여 커뮤니티의 발전에 기여하고자 합니다.

### 시사점
- **하이브리드 아키텍처의 가능성**: Mamba와 Transformer의 장점을 결합한 하이브리드 구조가 긴 컨텍스트 처리 및 추론 효율성 측면에서 매우 유망함을 보여줍니다.
- **실용적인 모델 압축 방법론**: 단순히 파라미터를 줄이는 것을 넘어, NAS를 통해 하드웨어 제약과 성능을 모두 고려한 최적의 아키텍처를 찾고 지식 증류로 성능을 복구하는 Minitron 기반의 압축 파이프라인은 LLM을 실제 서비스에 배포하는 데 있어 중요한 방법론을 제시합니다.
- **데이터 중심의 성능 향상**: 고품질의 합성 데이터를 생성하고, 정교한 데이터 커리큘럼을 적용하는 것이 모델의 최종 성능에 결정적인 역할을 한다는 점을 다시 한번 확인시켜 줍니다.

## 7. 한계점 및 향후 연구 방향
### 한계점 (논문에 명시되지 않았으나 추론 가능)
- **Mamba 구성 요소의 민감성**: Mamba 헤드를 pruning했을 때 심각한 성능 저하가 발생한 점으로 미루어 볼 때, Mamba 구조의 특정 구성 요소는 압축에 매우 민감하여 추가적인 연구가 필요해 보입니다.
- **다중 목표 최적화의 어려움**: 추론 능력과 채팅 능력을 동시에 최적화하는 데 어려움이 있어 결국 두 모델을 병합하는 방법을 사용했습니다. 이는 단일 정렬 프로세스로 여러 상충하는 목표를 달성하는 것이 여전히 도전적인 과제임을 시사합니다.

### 향후 연구 방향 (논문에 명시되지 않았으나 추론 가능)
- Mamba 구조에 대한 보다 정교한 압축 기술 개발
- 다양한 능력을 동시에 향상시킬 수 있는 새로운 정렬(alignment) 기법 연구
- 더 적은 계산 자원으로도 효율적인 사전 학습을 가능하게 하는 방법론 탐색

---

## Figure/Table 요약

- **Figure 1**: Nemotron Nano 2 (9B)와 Qwen3-8B의 정확도 및 처리량 비교. 주요 벤치마크에서 정확도는 비슷하거나 더 높으면서, 처리량은 최대 6.3배 높음을 시각적으로 보여줍니다.
- **Figure 2 & Table 1**: Nemotron-Nano-12B-v2-Base 모델의 아키텍처. 62개 레이어 중 6개가 attention이고 나머지는 Mamba-2와 FFN으로 구성된 하이브리드 구조와 상세 파라미터를 보여줍니다.
- **Figure 3**: 사전 학습의 3단계(Phase)에 걸친 데이터 혼합 비율. 초기에는 다양한 데이터를 사용하다가 후반부로 갈수록 고품질 데이터(코드, 수학, SFT 스타일 데이터 등)의 비중을 높이는 커리큘럼을 보여줍니다.
- **Table 5 & 6**: Nemotron Nano 12B 및 9B **베이스 모델**과 Qwen3, Gemma3 모델의 벤치마크 성능 비교. 특히 수학 및 코딩 능력에서 Nemotron이 우위를 보입니다.
- **Table 8**: 최종 **정렬 모델**의 성능 비교. AIME, MATH-500 등 고난도 추론 벤치마크에서 Qwen 모델들과 경쟁력 있는 점수를 기록했습니다.
- **Figure 5**: **사고 예산 제어** 기능의 개선 효과. 중단된 추론 데이터로 학습하기 전(a)과 후(b)를 비교하며, 학습 후에 모델이 제한된 예산 내에서도 훨씬 안정적이고 잘 구성된 답변을 생성함을 보여줍니다.
- **Table 10 & 11**: 모델 **압축(Pruning)** 과정에서의 아키텍처 탐색 및 데이터 혼합 비율 실험 결과. 최적의 아키텍처와 지식 증류 데이터 비율을 찾기 위한 과정을 요약합니다.

## Appendix 요약
- **Appendix A. Permissive Source Code Licenses**
    - 이 부록은 모델의 사전 학습에 사용된 **코드 데이터셋을 큐레이션하는 데 사용된 허용 가능한 소스 코드 라이선스 목록**을 상세히 기술합니다.
    - 라이선스가 없는 코드를 무단으로 사용하는 것을 방지하기 위해, Apache 2.0, MIT, BSD 계열 등 상업적으로도 비교적 자유롭게 사용할 수 있는 수백 개의 라이선스를 명시했습니다.
    - 이는 모델이 저작권 문제가 없는 "깨끗한" 코드 데이터로 학습되었음을 보장하기 위한 노력의 일환입니다. 각 라이선스는 ScanCode 툴킷의 식별 키와 함께 제공됩니다.