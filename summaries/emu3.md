## 1. 논문 정보

### 1.1. 제목
Emu3: Next-Token Prediction is All You Need (Emu3: 다음 토큰 예측이 당신에게 필요한 전부입니다)

### 1.2. 저자
Emu3 Team, BAAI (Beijing Academy of Artificial Intelligence)

---

## 2. 연구 개요

### 2.1. 연구 목적
이 논문의 핵심 목적은 **단일한 '다음 토큰 예측(Next-Token Prediction)' 패러다임**만으로 이미지/비디오 생성(Generation)과 이해(Perception)를 아우르는 고성능 멀티모달 모델을 구축할 수 있음을 증명하는 것입니다. 기존의 복잡한 구조(예: Diffusion, CLIP+LLM)에 의존하지 않고, 모든 양식(modality)을 이산적인 토큰(discrete token) 시퀀스로 변환하여 하나의 Transformer 모델로 처리함으로써, 모델 설계의 단순화와 성능의 극대화를 동시에 달성하고자 합니다.

### 2.2. 연구 배경
현재 멀티모달 AI 분야는 크게 두 가지 접근법이 지배하고 있습니다.
1.  **비전 생성(Vision Generation)**: Stable Diffusion (SDXL), Sora와 같은 **Diffusion 모델**이 사실적인 이미지와 비디오를 생성하며 SOTA(State-of-the-Art) 성능을 보여주고 있습니다.
2.  **비전-언어 인식(Vision-Language Perception)**: CLIP과 같은 비전 인코더와 LLM을 결합한 **구성적(Compositional) 접근법**(예: LLaVA)이 이미지 이해, VQA(Visual Question Answering) 등에서 뛰어난 성능을 보입니다.

이러한 분리된 접근법은 각 태스크에 특화되어 있지만, 범용적인 멀티모달 지능(General Multimodal Intelligence)으로 나아가는 데 한계가 있습니다. 이전의 통합 모델들은 Diffusion 모델을 LLM에 연결하거나, 태스크 특화 모델의 성능에 미치지 못하는 경우가 많았습니다. Emu3는 이러한 배경 속에서, 언어 모델의 성공을 이끈 'Next-Token Prediction'이라는 단순하고 확장 가능한 원리를 멀티모달 영역으로 가져와 그 유효성을 입증하고자 합니다.

---

## 3. 방법론 (Methodology)

### 3.1. 핵심 아이디어
Emu3의 모든 것은 **'토큰(Token)'**으로 귀결됩니다. 비디오, 이미지, 텍스트 등 모든 종류의 데이터를 이산적인 토큰의 시퀀스로 변환(Tokenize)합니다. 그 후, 거대한 단일 Transformer Decoder 모델을 사용하여 이 시퀀스에서 다음 토큰이 무엇일지 예측하도록 학습시킵니다. 생성된 토큰 시퀀스를 다시 원래의 데이터 형태로 변환(Detokenize)하면 이미지나 비디오, 텍스트가 결과물로 나옵니다. 이 접근법을 통해 생성과 이해라는 두 가지 과업을 별도의 아키텍처 없이 하나의 모델로 통합합니다.

### 3.2. 데이터 (Data)
Emu3는 언어, 이미지, 비디오 세 종류의 대규모 데이터를 사용하여 처음부터(from scratch) 학습되었습니다.
* **언어 데이터**: 고품질의 중국어 및 영어 코퍼스인 Aquila의 데이터를 사용했습니다.
* **이미지 데이터**: 웹 데이터, AI 생성 데이터, 자체 고품질 데이터를 혼합하여 사용했습니다. 해상도($512\times512$ 이상)와 미학 점수(LAION-AI aesthetic predictor 5.5 이상)를 기준으로 필터링했으며, Emu2와 GPT-4V를 활용해 고품질의 캡션을 생성하여 주석을 달았습니다.
* **비디오 데이터**: 풍경, 동물, 게임 등 다양한 카테고리의 비디오를 수집했습니다. 장면 분할, 텍스트 제거, 모션(optical flow) 필터링, 미학 점수 필터링 등 정교한 파이프라인을 거쳐 데이터를 정제했습니다. 캡션은 이미지 캡셔너를 비디오에 맞게 파인튜닝하여 생성했습니다.

### 3.3. Vision Tokenizer
SBER-MoVQGAN을 기반으로 훈련된 비전 토크나이저는 이미지나 비디오를 이산적인 토큰으로 변환하는 핵심 요소입니다.
* $512\times512$ 이미지 또는 $4\times512\times512$ 비디오 클립을 32,768개의 코드북에서 4096개의 토큰으로 인코딩합니다.
* 공간적으로 $8\times8$, 시간적으로 4배의 압축률을 가집니다.
* 기존 MoVQGAN 구조에 3D 컨볼루션 커널을 포함한 시간적 레이어를 추가하여 비디오 처리 능력을 강화했습니다.

### 3.4. 아키텍처 (Architecture)
Emu3는 Llama-2와 같은 표준 LLM 아키텍처를 거의 그대로 따릅니다.
* 주요 변경점은 임베딩 레이어를 확장하여 비전 토큰을 수용할 수 있도록 한 것입니다.
* RMSNorm, GQA(Grouped-Query Attention), SwiGLU 활성화 함수, RoPE(Rotary Positional Embeddings) 등 최신 LLM 기술들을 사용합니다.
* 총 80억(8B) 개의 파라미터를 가지며, 최대 131,072 토큰의 긴 컨텍스트 길이를 처리할 수 있습니다.

### 3.5. 사전 학습 (Pre-training)
* **데이터 형식**: `[BOS] caption text [SOV] meta text [SOT] {vision tokens} [EOV] [EOS]` 와 같은 특별한 토큰을 사용하여 텍스트와 비전 데이터를 하나의 문서처럼 구성합니다.
* **학습 목표**: 표준적인 Cross-Entropy Loss를 사용한 Next-Token Prediction입니다. 비전 토큰이 학습을 지배하는 것을 막기 위해 비전 토큰 관련 손실(loss)에 0.5의 가중치를 적용합니다.
* **2단계 학습**:
    1.  **1단계**: 비디오 데이터 없이 텍스트와 이미지 데이터만으로 5,120의 컨텍스트 길이로 학습을 시작합니다.
    2.  **2단계**: 비디오 데이터를 추가하고, 컨텍스트 길이를 131,072로 늘려 학습을 계속합니다.

### 3.6. 후속 학습 (Post-training)
사전 학습된 모델의 성능을 특정 태스크에 맞게 극대화하기 위해 후속 학습을 진행합니다.

* **비전 생성 (Vision Generation)**:
    * **Quality Fine-Tuning (QFT)**: 선별된 고품질 데이터만을 사용하여 생성물의 질을 높입니다. 이 단계에서는 비전 토큰에 대해서만 손실을 계산하여 학습합니다.
    * **Direct Preference Optimization (DPO)**: 인간의 선호도 데이터를 사용하여 모델을 정렬(align)합니다. 사용자가 선호하는 결과물(chosen)과 선호하지 않는 결과물(rejected) 쌍을 이용해 모델이 더 나은 결과물을 생성하도록 미세 조정합니다.

* **비전-언어 이해 (Vision-Language Understanding)**:
    * **1단계 (Image-to-text training)**: 이미지 이해 데이터와 언어 전용 데이터를 혼합하여 학습합니다.
    * **2단계 (Instruction tuning)**: 다양한 질문-답변 쌍 데이터를 사용하여 모델의 지시 수행 능력을 강화합니다.

---

## 4. 실험 및 결과

### 4.1. 실험 설정
* **이미지 생성**: MSCOCO-30K, GenEval, T2I-CompBench, DPG-Bench 등 자동화된 벤치마크와 인간 평가를 함께 진행했습니다.
* **비디오 생성**: VBench 벤치마크를 사용하여 13개의 SOTA 공개/비공개 모델과 성능을 비교했습니다.
* **비전-언어 이해**: SEEDBench-Img, OCRBench, MMVet 등 12개의 공개 벤치마크에서 평균 점수를 측정하여 LLaVA-1.6 등과 비교했습니다.

### 4.2. 주요 결과
Emu3는 여러 멀티모달 태스크에서 기존의 SOTA 모델들을 능가하는 성능을 보였습니다.

#### 4.2.1. 이미지 생성 (Image Generation)
* **인간 평가**: Emu3는 대표적인 오픈소스 Diffusion 모델인 **SDXL을 능가**했으며, DALL-E 3, MJ-v5.2와 동등한 수준의 선호도 점수를 기록했습니다.
* **자동화 벤치마크**: DPG-Bench(긴 프롬프트 이해 능력 평가)에서 SDXL을 능가하고 DALL-E 3와 비슷한 점수를 얻었습니다.
* **DPO 효과**: DPO를 적용한 Emu3-DPO 모델은 시각적 품질과 프롬프트 일치도 측면에서 인간 선호도 점수가 크게 향상되었습니다.

#### 4.2.2. 비디오 생성 (Video Generation)
* Emu3는 기본적으로 24 FPS의 5초 비디오를 생성할 수 있으며, 자기회귀(autoregressive) 방식으로 무한히 확장 가능합니다.
* VBench 벤치마크에서 Kling, Gen-3와 같은 최신 비공개 모델에는 약간 뒤처지지만, **대부분의 오픈소스 텍스트-투-비디오 모델을 능가**하는 경쟁력 있는 점수를 달성했습니다.
* 단순히 다음 프레임을 예측하는 방식으로 기존 비디오를 자연스럽게 연장하는 **미래 예측(Future Prediction)** 능력도 보여주었습니다.

#### 4.2.3. 비전-언어 이해 (Vision-Language Understanding)
* Emu3는 12개 벤치마크 평균 점수에서 널리 사용되는 **LLaVA-1.6을 능가**했습니다.
* 사전 학습된 비전 인코더(CLIP)나 LLM에 의존하지 않는 순수한 **인코더-프리(Encoder-free)** 방식임에도 불구하고, 인코더 기반 접근법들과 대등하거나 더 나은 성능을 달성했다는 점에서 큰 의의가 있습니다.

---

## 5. 분석 및 고찰

### 5.1. 결과 분석
Emu3의 성공은 'Next-Token Prediction'이라는 단순한 패러다임이 복잡한 멀티모달 데이터를 처리하고 생성하는 데 매우 효과적임을 시사합니다. 모든 것을 토큰으로 통합함으로써, 모델은 서로 다른 양식 간의 복잡한 관계를 내재적으로 학습할 수 있습니다. 특히 DPO를 시각 생성에 성공적으로 적용한 것은, 언어 모델의 정렬 기술이 멀티모달 영역에서도 유효함을 보여주는 중요한 사례입니다. 이는 모델 개발의 복잡성을 크게 줄이면서도 성능을 높일 수 있는 새로운 길을 제시합니다.

### 5.2. Figure/Table 요약
* **Figure 1**: Emu3의 핵심 아키텍처를 도식화하여, 모든 입력(비디오, 이미지, 텍스트)을 토큰화하고 단일 Transformer Decoder를 통해 다음 토큰을 예측하는 과정을 보여줍니다.
* **Figure 2**: Emu3("Ours")가 이미지 생성(vs SDXL), 비전-언어 이해(vs LLaVA-1.6), 비디오 생성(vs OpenSora 1.2) 세 가지 핵심 분야에서 경쟁 모델들을 능가했음을 보여주는 성능 비교 막대그래프입니다.
* **Figure 3**: 비디오와 이미지의 원본과 Emu3의 Vision Tokenizer를 통해 재구성된 결과를 비교하며, 토크나이저의 높은 복원 품질을 보여줍니다.
* **Figure 4**: Emu3가 생성한 다양한 스타일과 주제의 텍스트-투-이미지 결과물 예시입니다.
* **Figure 5 & 6**: 인간 평가 결과를 보여줍니다. Figure 5는 Emu3-DPO가 다른 유명 모델들과 비교하여 높은 선호도를 얻었음을, Figure 6은 DPO 튜닝이 시각적 품질과 프롬프트 이해도를 모두 향상시켰음을 보여줍니다.
* **Figure 7 & 8**: Emu3의 텍스트-투-비디오 생성 및 기존 비디오 연장(미래 예측)의 질적 예시를 보여줍니다.
* **Table 4, 5, 6**: 각각 이미지 생성, 비디오 생성, 비전-언어 이해 벤치마크에서 Emu3와 여러 SOTA 모델들의 정량적 성능을 상세히 비교한 표입니다.
* **Table 7, 8**: 이미지 생성 벤치마크인 GenEval, T2I-CompBench, DPG-Bench에서의 세부 점수를 나타냅니다.

---

## 6. 결론 및 시사점

### 6.1. 결론
Emu3는 'Next-Token Prediction'이라는 단일 원칙만으로 멀티모달 생성과 인식을 모두 정복할 수 있다는 강력한 증거를 제시합니다. 이산적인 공간에서 이미지, 비디오, 텍스트를 토큰으로 처리하고 단일 Transformer로 학습함으로써, Diffusion이나 구성적 접근법에 대한 의존성을 없애고 SDXL, LLaVA-1.6과 같은 기존의 강력한 태스크 특화 모델들을 능가했습니다. 이는 범용 멀티모달 지능(General Multimodal Intelligence), 나아가 AGI(Artificial General Intelligence)를 향한 유망한 경로임을 시사합니다.

### 6.2. 시사점 (Implications)
* **아키텍처의 통합**: 복잡하고 이질적인 멀티모달 아키텍처를 단순하고 확장 가능한 단일 Transformer 구조로 통합할 수 있는 가능성을 열었습니다.
* **확장성**: 언어 모델에서 검증된 것처럼, 데이터와 모델 크기를 늘리는 스케일링(scaling)을 통해 성능을 지속적으로 향상시킬 수 있는 잠재력을 가집니다.
* **커뮤니티 기여**: 핵심 기술과 모델(특히 Vision Tokenizer)을 오픈소스로 공개하여 관련 분야의 후속 연구를 촉진합니다.

### 6.3. 한계점 및 향후 연구 방향 (Limitations & Future Work)
* **자동 평가와 인간 선호의 불일치**: DPO를 적용했을 때 인간 선호도 점수는 올랐지만 일부 자동화 벤치마크 점수는 하락하는 경향이 있었습니다. 이는 현재 자동 평가 지표가 인간의 미적 선호도를 완벽히 반영하지 못함을 시사합니다.
* **계산 비용**: 131,072라는 긴 컨텍스트 길이를 처리하기 위한 사전 학습에는 막대한 계산 자원이 필요했을 것으로 추정됩니다.
* **향후 연구**: 모델을 더 큰 규모로 확장하고, 더 다양하고 품질 좋은 데이터로 학습시키는 연구가 필요합니다. 또한, 현재의 토크나이저를 개선하거나 새로운 양식(예: 오디오, 3D)을 토큰 시퀀스에 통합하는 방향으로 연구를 확장할 수 있습니다.

---

## 7. Appendix 요약

### 7.1. 데이터셋 상세 (A)
* 비디오 데이터셋 필터링 후 남은 클립들의 길이 분포와 모션(flow score) 분포를 시각화하여 데이터의 특성을 보여줍니다.

### 7.2. 평가 상세 (B)
* **이미지 생성 평가**: MSCOCO, GenEval, T2I CompBench, DPG-bench 등 각 벤치마크 평가 시 사용된 상세한 하이퍼파라미터(Top-k, Top-p, 해상도, guidance scale 등)를 명시했습니다. 예를 들어, MSCOCO 평가에서는 3만 개의 프롬프트를 샘플링하고 zero-shot FID 점수를 계산했다고 설명합니다.
* **결과 소스**: 다른 모델들의 성능 점수를 어떤 논문이나 소스에서 가져왔는지 출처를 명확히 밝힙니다.

### 7.3. 후처리 (Post Processing)
* 생성된 비디오의 시간적 일관성과 시각적 품질을 더욱 향상시키기 위해 **별도의 후처리 모델을 사용했음**을 밝힙니다.
* **비디오 안정화(Video Stabilization)**: Stable Video Diffusion의 Temporal VAE를 기반으로 한 모델을 사용합니다.
* **초해상도(Super-Resolution)**: 4배 업샘플링이 가능한 공간-시간적 U-Net 모델을 구현하여 사용합니다.
* 이는 논문에서 보여주는 최종 비디오 결과물이 Emu3의 순수 출력물에 전문적인 후처리를 더한 결과임을 의미하는 중요한 정보입니다.

### 7.4. 멀티모달 이해를 위한 질적 예시 (C)
* Emu3가 멀티모달 이해 태스크를 얼마나 잘 수행하는지 실제 예시를 통해 보여줍니다.
    * 브루클린 다리 사진을 보고 랜드마크를 정확히 묘사합니다.
    * 수식 이미지를 보고 해당하는 LaTeX 코드를 생성합니다.
    * 해양 생태계 먹이 사슬 다이어그램을 이해하고, 특정 개체수 변화가 미칠 영향에 대한 객관식 질문에 정확히 답합니다.
    * 잡지 표지를 보고 제목, 내용, 등장인물, 복장 등 매우 상세한 수준의 OCR 및 장면 묘사를 수행합니다.